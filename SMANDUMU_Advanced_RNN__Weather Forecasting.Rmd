---
title: "Advanced_RNN:Weather Forecasting"
author: "SMANDUMU"
output:
  rmdformats::html_clean:
    highlight: kate
---


```{r setup, echo=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)
```


Improve weather forecasting for the problem

```{r, message=FALSE}
library(tibble)
library(readr)
library(keras)
data <- read_csv("jena_climate_2009_2016.csv")
glimpse(data)
```

Here is the plot of temperature (in degrees Celsius) over time. On this plot, you can clearly see the yearly periodicity of temperature.

```{r}
library(ggplot2)
ggplot(data, aes(x = 1:nrow(data), y = `T (degC)`)) + geom_line()
```

We clearly see the yearly periodicity of temperature.

```{r}
ggplot(data[1:1440,], aes(x = 1:1440, y = `T (degC)`)) + geom_line()
```

We can see daily periodicity, especially evident for the last 4 days

## Preparing the Data

We use the following parameter values:

* `lookback = 1440`, i.e. our observations will go back 10 days.
* `steps = 6`, i.e. our observations will be sampled at one data point per hour.
* `delay = 144`, i.e. our targets will be 24 hours in the future.

First, you'll convert the R data frame which we read earlier into a matrix of floating point values (we'll discard the first column which included a text timestamp):

```{r}
data <- data.matrix(data[,-1])
```

We use the first 40000 timesteps as training data, so compute the mean and standard deviation for normalization only on this fraction of the data.

```{r}
train_data <- data[1:40000,]
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
data <- scale(data, center = mean, scale = std)
```

The data generator you will use.Yields a list where `samples` is one batch of input data and `targets` is the corresponding array of target temperatures.

```{r}
generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 128, step = 6) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), 
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
                     
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]] - 1, 
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay,2]
    }            
    
    list(samples, targets)
  }
}
```

Now we use the abstract `generator` function to instantiate three generators: one for training, one for validation, and one for testing.

```{r}
lookback <- 1440
step <- 6
delay <- 144
batch_size <- 128
train_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 20000,
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)
val_gen = generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 20001,
  max_index = 30000,
  step = step,
  batch_size = batch_size
)
test_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 30001,
  max_index = 40000,
  step = step,
  batch_size = batch_size
)
# This is how many steps to draw from `val_gen`
# in order to see the whole validation set:
val_steps <- (22000 - 15001 - lookback) / batch_size
  # This is how many steps to draw from `test_gen`
# in order to see the whole test set:
test_steps <- (nrow(data) - 22001 - lookback) / batch_size
```

## Non Machine Learning Baseline Model

```{r}
evaluate_naive_method <- function() {
  batch_maes <- c()
  for (step in 1:val_steps) {
    c(samples, targets) %<-% val_gen()
    preds <- samples[,dim(samples)[[2]],2]
    mae <- mean(abs(preds - targets))
    batch_maes <- c(batch_maes, mae)
  }
  print(mean(batch_maes))
}
```

## Dense Layered Model

```{r, echo=TRUE, results='hide'}
library(keras)

model <- keras_model_sequential() %>% 
  layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

## Baseline Recurrent Model:gru

```{r}

model <- keras_model_sequential() %>% 
  layer_gru(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_dense(units = 1)
model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)
history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 50,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

```{r}
plot(history)
```

## Improvements to Model : Recurrent Model with Dropout and Stacked gru Layers

```{r, echo=TRUE, results='hide'}
model <- keras_model_sequential() %>% 
  layer_gru(units = 64, 
            dropout = 0.2,                                  
            recurrent_dropout = 0.6,
            return_sequences = TRUE,                        
            input_shape = list(NULL, dim(data)[[-1]])) %>%
  layer_gru(units = 128,                             
            activation = "relu",                         
            dropout = 0.2, 
            recurrent_dropout = 0.6) %>%                  
    layer_dense(units = 32, activation = "relu") %>%    
  # Added dense layer 32 units
  layer_dense(units = 64, activation = "relu") %>%  
  # Added dense layer 64 units
  layer_dense(units = 1)
model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)
history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

```{r}
plot(history)
```

Review of test results:

```{r}
history_test <- model %>% fit_generator(
  test_gen,
  steps_per_epoch = 100,
  epochs = 10,
   validation_steps = test_steps
)
```

```{r}
plot(history_test)
```

The *MAE* from basic model is *~ 0.27* 
The loss of test acheived *~ 0 * at Epoch 0
